{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1w9AYcq5R1L"
   },
   "source": [
    "# Big Data Management Project 3:\n",
    "## Analysing Flight Interconnected Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /usr/local/spark/python (from delta-spark) (3.5.3)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.20.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n",
      "Requirement already satisfied: graphframes in /opt/conda/lib/python3.11/site-packages (0.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from graphframes) (1.26.4)\n",
      "Requirement already satisfied: nose in /opt/conda/lib/python3.11/site-packages (from graphframes) (1.3.7)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "!pip install delta-spark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "!pip install graphframes\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# 🔧 Spark Session with GraphFrames\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"FlightGraphAnalysis\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder, extra_packages=[\"graphframes:graphframes:0.8.4-spark3.5-s_2.12\"]\n",
    ").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark._sc.defaultParallelism)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "\n",
    "import graphframes as gf # Import the module after installing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download/extraction:\n",
    "\n",
    "##### To simplify access to the dataset, we added a script to automatically check for the presence of the required data file (2009.csv). If the file is not found, the script downloads a zipped version from Google Drive (link from Moodle) using gdown, then extracts it into the right directory. This approach makes the data available locally without downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /opt/conda/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "input/2009.csv already exists. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "zip_filename = \"input/2009.csv.zip\"\n",
    "csv_filename = \"input/2009.csv\"\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    print(f\"{csv_filename} already exists. Nothing to do.\")\n",
    "\n",
    "elif os.path.exists(zip_filename):\n",
    "    print(f\"{csv_filename} not found, but {zip_filename} exists. Unzipping...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"input/.\")\n",
    "    print(\"Unzipping complete.\")\n",
    "\n",
    "else:\n",
    "    print(\"File is missing, importing from Google Drive\")\n",
    "    !gdown 1trFtRCe3xPBLr90hIWBF__OqppEnJPR_ -O input/\n",
    "    print(\"File downloaded, going to unzip\")\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"input/.\")\n",
    "    print(\"Unzipping complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📂 Load Data and selecting\n",
    "\n",
    "##### We loaded the flight dataset and selected only the necessary columns. This helped reduce memory usage and speeds up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+--------+\n",
      "|ORIGIN|DEST|   FL_DATE|DISTANCE|\n",
      "+------+----+----------+--------+\n",
      "|   DCA| EWR|2009-01-01|   199.0|\n",
      "|   EWR| IAD|2009-01-01|   213.0|\n",
      "|   EWR| DCA|2009-01-01|   199.0|\n",
      "|   DCA| EWR|2009-01-01|   199.0|\n",
      "|   IAD| EWR|2009-01-01|   213.0|\n",
      "|   ATL| EWR|2009-01-01|   745.0|\n",
      "|   CLE| ATL|2009-01-01|   554.0|\n",
      "|   DCA| EWR|2009-01-01|   199.0|\n",
      "|   EWR| DCA|2009-01-01|   199.0|\n",
      "|   EWR| DCA|2009-01-01|   199.0|\n",
      "|   DCA| EWR|2009-01-01|   199.0|\n",
      "|   EWR| DCA|2009-01-01|   199.0|\n",
      "|   CLE| DCA|2009-01-01|   310.0|\n",
      "|   DCA| EWR|2009-01-01|   199.0|\n",
      "|   ORD| EWR|2009-01-01|   719.0|\n",
      "|   EWR| ORD|2009-01-01|   719.0|\n",
      "|   ORD| EWR|2009-01-01|   719.0|\n",
      "|   EWR| ORD|2009-01-01|   719.0|\n",
      "|   ORD| EWR|2009-01-01|   719.0|\n",
      "|   EWR| ORD|2009-01-01|   719.0|\n",
      "+------+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure the file is inside the container path (e.g., /app/input/flights.csv)\n",
    "#df = spark.read.csv(\"/content/2009.csv\", header=True, inferSchema=True)\n",
    "#display(df.limit(20))\n",
    "\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(\"input/2009.csv\")\n",
    "      # choose only rows we are interested in\n",
    "      .select(\"ORIGIN\", \"DEST\", \"FL_DATE\", \"DISTANCE\"))\n",
    "\n",
    "df.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph\n",
    "##### In this graph each airport is represented as a vertex and each flight as a directed edge. The `airports_df` shows unique airport codes from both origin and destination columns. The `edges_df` contains flight connections with `src` (origin) and `dst` (destination). Using these, we constructed the `flights_graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airports total: 296\n",
      "Flights total: 6429338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The airports from data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th></tr>\n",
       "<tr><td>DCA</td></tr>\n",
       "<tr><td>CLT</td></tr>\n",
       "<tr><td>AVL</td></tr>\n",
       "<tr><td>BTV</td></tr>\n",
       "<tr><td>BNA</td></tr>\n",
       "<tr><td>XNA</td></tr>\n",
       "<tr><td>GJT</td></tr>\n",
       "<tr><td>LGB</td></tr>\n",
       "<tr><td>MBS</td></tr>\n",
       "<tr><td>TVC</td></tr>\n",
       "<tr><td>LYH</td></tr>\n",
       "<tr><td>CLL</td></tr>\n",
       "<tr><td>ELM</td></tr>\n",
       "<tr><td>BGM</td></tr>\n",
       "<tr><td>TYS</td></tr>\n",
       "<tr><td>BHM</td></tr>\n",
       "<tr><td>DTW</td></tr>\n",
       "<tr><td>PHX</td></tr>\n",
       "<tr><td>DLH</td></tr>\n",
       "<tr><td>FCA</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+\n",
       "| id|\n",
       "+---+\n",
       "|DCA|\n",
       "|CLT|\n",
       "|AVL|\n",
       "|BTV|\n",
       "|BNA|\n",
       "|XNA|\n",
       "|GJT|\n",
       "|LGB|\n",
       "|MBS|\n",
       "|TVC|\n",
       "|LYH|\n",
       "|CLL|\n",
       "|ELM|\n",
       "|BGM|\n",
       "|TYS|\n",
       "|BHM|\n",
       "|DTW|\n",
       "|PHX|\n",
       "|DLH|\n",
       "|FCA|\n",
       "+---+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flights between airports from data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>src</th><th>dst</th></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>IAD</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>IAD</td><td>EWR</td></tr>\n",
       "<tr><td>ATL</td><td>EWR</td></tr>\n",
       "<tr><td>CLE</td><td>ATL</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>CLE</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+---+\n",
       "|src|dst|\n",
       "+---+---+\n",
       "|DCA|EWR|\n",
       "|EWR|IAD|\n",
       "|EWR|DCA|\n",
       "|DCA|EWR|\n",
       "|IAD|EWR|\n",
       "|ATL|EWR|\n",
       "|CLE|ATL|\n",
       "|DCA|EWR|\n",
       "|EWR|DCA|\n",
       "|EWR|DCA|\n",
       "|DCA|EWR|\n",
       "|EWR|DCA|\n",
       "|CLE|DCA|\n",
       "|DCA|EWR|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "+---+---+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 👥 Vertices: Airports , needs to contain id column\n",
    "airports_df = df.select(F.col(\"ORIGIN\").alias(\"id\")).union(\n",
    "    df.select(F.col(\"DEST\").alias(\"id\"))\n",
    ").distinct()\n",
    "\n",
    "# ✈️ Edges: Flights , Needs to contain src and dst columns\n",
    "edges_df = df.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\")\n",
    ")\n",
    "\n",
    "# 📊 GraphFrame\n",
    "flights_graph = gf.GraphFrame(airports_df, edges_df)\n",
    "print(\"Airports total:\", flights_graph.vertices.count())\n",
    "print(\"Flights total:\", flights_graph.edges.count())\n",
    "airports_df.cache()\n",
    "edges_df.cache()\n",
    "display(flights_graph)\n",
    "print(\"The airports from data: \") \n",
    "display(flights_graph.vertices)\n",
    "print(\"The flights between airports from data: \")\n",
    "display(flights_graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom in-degree, out-degree, degree, triangle count\n",
    "\n",
    "##### First, we computed the in-degree by counting how many flights arrive at each airport (dst). Then, we calculated the out-degree by counting how many flights depart from each airport (src). Finally, we combined both to compute the total degree for each airport by summing in-degree and out-degree. Missing values were handled using coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+-----------+\n",
      "| id|inDegree|outDegree|totalDegree|\n",
      "+---+--------+---------+-----------+\n",
      "|ABE|    4037|     4034|       8071|\n",
      "|ABI|    2490|     2490|       4980|\n",
      "|ABQ|   35577|    35582|      71159|\n",
      "|ABY|     997|      995|       1992|\n",
      "|ACK|     343|      342|        685|\n",
      "|ACT|    1052|     1053|       2105|\n",
      "|ACV|    3364|     3370|       6734|\n",
      "|ACY|     522|      522|       1044|\n",
      "|ADK|     103|      103|        206|\n",
      "|ADQ|     631|      631|       1262|\n",
      "|AEX|    2948|     2947|       5895|\n",
      "|AGS|    3106|     3107|       6213|\n",
      "|AKN|      77|       77|        154|\n",
      "|ALB|   12020|    12018|      24038|\n",
      "|ALO|     331|      330|        661|\n",
      "|AMA|    6649|     6649|      13298|\n",
      "|ANC|   17788|    17791|      35579|\n",
      "|ASE|    4708|     4701|       9409|\n",
      "|ATL|  417457|   417449|     834906|\n",
      "|ATW|    5306|     5303|      10609|\n",
      "+---+--------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In-degree\n",
    "in_degree_df = edges_df.groupBy(\"dst\").count() \\\n",
    "    .withColumnRenamed(\"dst\", \"id\") \\\n",
    "    .withColumnRenamed(\"count\", \"inDegree\")\n",
    "\n",
    "# Out-degree\n",
    "out_degree_df = edges_df.groupBy(\"src\").count() \\\n",
    "    .withColumnRenamed(\"src\", \"id\") \\\n",
    "    .withColumnRenamed(\"count\", \"outDegree\")\n",
    "\n",
    "# Total degree (merge in & out)\n",
    "degree_df = in_degree_df.join(out_degree_df, on=\"id\", how=\"full_outer\") \\\n",
    "    .withColumn(\"inDegree\", F.coalesce(F.col(\"inDegree\"), F.lit(0))) \\\n",
    "    .withColumn(\"outDegree\", F.coalesce(F.col(\"outDegree\"), F.lit(0))) \\\n",
    "    .withColumn(\"totalDegree\", F.col(\"inDegree\") + F.col(\"outDegree\"))\n",
    "\n",
    "degree_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the number of unique triangles in the graph by joining edges to form two-step paths (A → B → C) and then checking if a closing edge (C → A) exists. To avoid duplicate counting, we sorted and filtered nodes (A < B < C). The result of Q2 - total triangles in the graph is 16015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triangles in the graph: 16015\n",
      "+---+---+---+\n",
      "|A  |B  |C  |\n",
      "+---+---+---+\n",
      "|IAH|JAX|MIA|\n",
      "|IAH|JAX|TPA|\n",
      "|IAH|JAX|STL|\n",
      "|IAH|JAX|MEM|\n",
      "|IAH|JAX|MSP|\n",
      "|IAH|JAX|LGA|\n",
      "|IAH|JAX|LAS|\n",
      "|IAH|JAX|ORF|\n",
      "|IAH|JAX|JFK|\n",
      "|IAH|JAX|PHL|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Edges (a -> b) ensuring a < b to avoid double counting\n",
    "edges_filtered = edges_df.select(\n",
    "    F.least(\"src\", \"dst\").alias(\"src\"),\n",
    "    F.greatest(\"src\", \"dst\").alias(\"dst\")\n",
    ").distinct().cache()\n",
    "\n",
    "# Step 2: Find two-edge paths: (a -> b) join (b -> c)\n",
    "paths_two = edges_filtered.alias(\"e1\").join(\n",
    "    edges_filtered.alias(\"e2\"),\n",
    "    F.col(\"e1.dst\") == F.col(\"e2.src\")\n",
    ").select(\n",
    "    F.col(\"e1.src\").alias(\"A\"),\n",
    "    F.col(\"e1.dst\").alias(\"B\"),\n",
    "    F.col(\"e2.dst\").alias(\"C\")\n",
    ").filter(\"A < B AND B < C\")\n",
    "\n",
    "# Step 3: Close the triangles by checking (a,c) exists\n",
    "triangles = paths_two.join(\n",
    "    edges_filtered.alias(\"e3\"),\n",
    "    (F.col(\"e3.src\") == F.col(\"A\")) & (F.col(\"e3.dst\") == F.col(\"C\"))\n",
    ").select(\"A\", \"B\", \"C\").distinct()\n",
    "\n",
    "# Count total triangles\n",
    "triangle_count = triangles.count()\n",
    "print(f\"Total triangles in the graph: {triangle_count}\")\n",
    "\n",
    "# Display some triangles for verification\n",
    "triangles.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of triangles in graph (again): 16015\n"
     ]
    }
   ],
   "source": [
    "#Total triangles\n",
    "print(f\"Total number of triangles in graph (again): {triangle_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "#### Centrality Measure (custom) – Degree Centrality \n",
    "##### Degree Centrality Normalization:  We normalized the totalDegree by dividing it by the total number of airports in the data. Here we can see that the resulting values can be compared proportionally across the entire graph. This can help to understand how much each airport influences the flight network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>inDegree</th><th>outDegree</th><th>totalDegree</th><th>degreeCentrality</th></tr>\n",
       "<tr><td>ATL</td><td>417457</td><td>417449</td><td>834906</td><td>2820.6283783783783</td></tr>\n",
       "<tr><td>ORD</td><td>313769</td><td>313848</td><td>627617</td><td>2120.3277027027025</td></tr>\n",
       "<tr><td>DFW</td><td>264398</td><td>264396</td><td>528794</td><td>1786.4662162162163</td></tr>\n",
       "<tr><td>DEN</td><td>235700</td><td>235675</td><td>471375</td><td>1592.4831081081081</td></tr>\n",
       "<tr><td>LAX</td><td>192916</td><td>192879</td><td>385795</td><td>1303.3614864864865</td></tr>\n",
       "<tr><td>PHX</td><td>183491</td><td>183502</td><td>366993</td><td>1239.8412162162163</td></tr>\n",
       "<tr><td>IAH</td><td>182088</td><td>182097</td><td>364185</td><td>1230.3547297297298</td></tr>\n",
       "<tr><td>LAS</td><td>153984</td><td>153993</td><td>307977</td><td>1040.462837837838</td></tr>\n",
       "<tr><td>DTW</td><td>152075</td><td>152081</td><td>304156</td><td>1027.554054054054</td></tr>\n",
       "<tr><td>SFO</td><td>136532</td><td>136488</td><td>273020</td><td>922.3648648648649</td></tr>\n",
       "<tr><td>SLC</td><td>131674</td><td>131694</td><td>263368</td><td>889.7567567567568</td></tr>\n",
       "<tr><td>MCO</td><td>120936</td><td>120944</td><td>241880</td><td>817.1621621621622</td></tr>\n",
       "<tr><td>MSP</td><td>119759</td><td>119732</td><td>239491</td><td>809.0912162162163</td></tr>\n",
       "<tr><td>JFK</td><td>119571</td><td>119574</td><td>239145</td><td>807.9222972972973</td></tr>\n",
       "<tr><td>EWR</td><td>118602</td><td>118602</td><td>237204</td><td>801.3648648648649</td></tr>\n",
       "<tr><td>CLT</td><td>116640</td><td>116650</td><td>233290</td><td>788.1418918918919</td></tr>\n",
       "<tr><td>BOS</td><td>110463</td><td>110460</td><td>220923</td><td>746.3614864864865</td></tr>\n",
       "<tr><td>SEA</td><td>100922</td><td>100948</td><td>201870</td><td>681.9932432432432</td></tr>\n",
       "<tr><td>BWI</td><td>100928</td><td>100923</td><td>201851</td><td>681.9290540540541</td></tr>\n",
       "<tr><td>LGA</td><td>100323</td><td>100334</td><td>200657</td><td>677.8952702702703</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+--------+---------+-----------+------------------+\n",
       "| id|inDegree|outDegree|totalDegree|  degreeCentrality|\n",
       "+---+--------+---------+-----------+------------------+\n",
       "|ATL|  417457|   417449|     834906|2820.6283783783783|\n",
       "|ORD|  313769|   313848|     627617|2120.3277027027025|\n",
       "|DFW|  264398|   264396|     528794|1786.4662162162163|\n",
       "|DEN|  235700|   235675|     471375|1592.4831081081081|\n",
       "|LAX|  192916|   192879|     385795|1303.3614864864865|\n",
       "|PHX|  183491|   183502|     366993|1239.8412162162163|\n",
       "|IAH|  182088|   182097|     364185|1230.3547297297298|\n",
       "|LAS|  153984|   153993|     307977| 1040.462837837838|\n",
       "|DTW|  152075|   152081|     304156| 1027.554054054054|\n",
       "|SFO|  136532|   136488|     273020| 922.3648648648649|\n",
       "|SLC|  131674|   131694|     263368| 889.7567567567568|\n",
       "|MCO|  120936|   120944|     241880| 817.1621621621622|\n",
       "|MSP|  119759|   119732|     239491| 809.0912162162163|\n",
       "|JFK|  119571|   119574|     239145| 807.9222972972973|\n",
       "|EWR|  118602|   118602|     237204| 801.3648648648649|\n",
       "|CLT|  116640|   116650|     233290| 788.1418918918919|\n",
       "|BOS|  110463|   110460|     220923| 746.3614864864865|\n",
       "|SEA|  100922|   100948|     201870| 681.9932432432432|\n",
       "|BWI|  100928|   100923|     201851| 681.9290540540541|\n",
       "|LGA|  100323|   100334|     200657| 677.8952702702703|\n",
       "+---+--------+---------+-----------+------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "degree_centrality = degree_df.withColumn(\"degreeCentrality\", F.col(\"totalDegree\") / airports_df.count())\n",
    "display(degree_centrality.orderBy(\"degreeCentrality\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[NESTED_AGGREGATE_FUNCTION] It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;\nAggregate [dst#4201], [dst#4201, sum((rank#4474 / cast(count(src#4200) as double))) AS contrib#4520]\n+- Join LeftOuter, (src#4200 = id#4166)\n   :- Project [ORIGIN#20 AS src#4200, DEST#21 AS dst#4201]\n   :  +- Project [ORIGIN#20, DEST#21, FL_DATE#17, DISTANCE#38]\n   :     +- Relation [FL_DATE#17,OP_CARRIER#18,OP_CARRIER_FL_NUM#19,ORIGIN#20,DEST#21,CRS_DEP_TIME#22,DEP_TIME#23,DEP_DELAY#24,TAXI_OUT#25,WHEELS_OFF#26,WHEELS_ON#27,TAXI_IN#28,CRS_ARR_TIME#29,ARR_TIME#30,ARR_DELAY#31,CANCELLED#32,CANCELLATION_CODE#33,DIVERTED#34,CRS_ELAPSED_TIME#35,ACTUAL_ELAPSED_TIME#36,AIR_TIME#37,DISTANCE#38,CARRIER_DELAY#39,WEATHER_DELAY#40,... 4 more fields] csv\n   +- Project [id#4166, 0.0033783783783783786 AS rank#4474]\n      +- Deduplicate [id#4166]\n         +- Union false, false\n            :- Project [ORIGIN#4480 AS id#4166]\n            :  +- Project [ORIGIN#4480, DEST#4481, FL_DATE#4477, DISTANCE#4498]\n            :     +- Relation [FL_DATE#4477,OP_CARRIER#4478,OP_CARRIER_FL_NUM#4479,ORIGIN#4480,DEST#4481,CRS_DEP_TIME#4482,DEP_TIME#4483,DEP_DELAY#4484,TAXI_OUT#4485,WHEELS_OFF#4486,WHEELS_ON#4487,TAXI_IN#4488,CRS_ARR_TIME#4489,ARR_TIME#4490,ARR_DELAY#4491,CANCELLED#4492,CANCELLATION_CODE#4493,DIVERTED#4494,CRS_ELAPSED_TIME#4495,ACTUAL_ELAPSED_TIME#4496,AIR_TIME#4497,DISTANCE#4498,CARRIER_DELAY#4499,WEATHER_DELAY#4500,... 4 more fields] csv\n            +- Project [DEST#4174 AS id#4168]\n               +- Project [ORIGIN#4173, DEST#4174, FL_DATE#4170, DISTANCE#4191]\n                  +- Relation [FL_DATE#4170,OP_CARRIER#4171,OP_CARRIER_FL_NUM#4172,ORIGIN#4173,DEST#4174,CRS_DEP_TIME#4175,DEP_TIME#4176,DEP_DELAY#4177,TAXI_OUT#4178,WHEELS_OFF#4179,WHEELS_ON#4180,TAXI_IN#4181,CRS_ARR_TIME#4182,ARR_TIME#4183,ARR_DELAY#4184,CANCELLED#4185,CANCELLATION_CODE#4186,DIVERTED#4187,CRS_ELAPSED_TIME#4188,ACTUAL_ELAPSED_TIME#4189,AIR_TIME#4190,DISTANCE#4191,CARRIER_DELAY#4192,WEATHER_DELAY#4193,... 4 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m ranks \u001b[38;5;241m=\u001b[39m airports_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m N))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m     12\u001b[0m     contribs \u001b[38;5;241m=\u001b[39m \u001b[43medges_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mranks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mranks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontrib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     ranks \u001b[38;5;241m=\u001b[39m contribs\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m damping) \u001b[38;5;241m/\u001b[39m N \u001b[38;5;241m+\u001b[39m damping \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrib\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m.\u001b[39mselect(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m.\u001b[39munion(airports_df\u001b[38;5;241m.\u001b[39mjoin(contribs, airports_df\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m==\u001b[39m contribs\u001b[38;5;241m.\u001b[39mdst, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_anti\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m                \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mlit((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m damping) \u001b[38;5;241m/\u001b[39m N)))\n\u001b[1;32m     21\u001b[0m display(ranks\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [NESTED_AGGREGATE_FUNCTION] It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;\nAggregate [dst#4201], [dst#4201, sum((rank#4474 / cast(count(src#4200) as double))) AS contrib#4520]\n+- Join LeftOuter, (src#4200 = id#4166)\n   :- Project [ORIGIN#20 AS src#4200, DEST#21 AS dst#4201]\n   :  +- Project [ORIGIN#20, DEST#21, FL_DATE#17, DISTANCE#38]\n   :     +- Relation [FL_DATE#17,OP_CARRIER#18,OP_CARRIER_FL_NUM#19,ORIGIN#20,DEST#21,CRS_DEP_TIME#22,DEP_TIME#23,DEP_DELAY#24,TAXI_OUT#25,WHEELS_OFF#26,WHEELS_ON#27,TAXI_IN#28,CRS_ARR_TIME#29,ARR_TIME#30,ARR_DELAY#31,CANCELLED#32,CANCELLATION_CODE#33,DIVERTED#34,CRS_ELAPSED_TIME#35,ACTUAL_ELAPSED_TIME#36,AIR_TIME#37,DISTANCE#38,CARRIER_DELAY#39,WEATHER_DELAY#40,... 4 more fields] csv\n   +- Project [id#4166, 0.0033783783783783786 AS rank#4474]\n      +- Deduplicate [id#4166]\n         +- Union false, false\n            :- Project [ORIGIN#4480 AS id#4166]\n            :  +- Project [ORIGIN#4480, DEST#4481, FL_DATE#4477, DISTANCE#4498]\n            :     +- Relation [FL_DATE#4477,OP_CARRIER#4478,OP_CARRIER_FL_NUM#4479,ORIGIN#4480,DEST#4481,CRS_DEP_TIME#4482,DEP_TIME#4483,DEP_DELAY#4484,TAXI_OUT#4485,WHEELS_OFF#4486,WHEELS_ON#4487,TAXI_IN#4488,CRS_ARR_TIME#4489,ARR_TIME#4490,ARR_DELAY#4491,CANCELLED#4492,CANCELLATION_CODE#4493,DIVERTED#4494,CRS_ELAPSED_TIME#4495,ACTUAL_ELAPSED_TIME#4496,AIR_TIME#4497,DISTANCE#4498,CARRIER_DELAY#4499,WEATHER_DELAY#4500,... 4 more fields] csv\n            +- Project [DEST#4174 AS id#4168]\n               +- Project [ORIGIN#4173, DEST#4174, FL_DATE#4170, DISTANCE#4191]\n                  +- Relation [FL_DATE#4170,OP_CARRIER#4171,OP_CARRIER_FL_NUM#4172,ORIGIN#4173,DEST#4174,CRS_DEP_TIME#4175,DEP_TIME#4176,DEP_DELAY#4177,TAXI_OUT#4178,WHEELS_OFF#4179,WHEELS_ON#4180,TAXI_IN#4181,CRS_ARR_TIME#4182,ARR_TIME#4183,ARR_DELAY#4184,CANCELLED#4185,CANCELLATION_CODE#4186,DIVERTED#4187,CRS_ELAPSED_TIME#4188,ACTUAL_ELAPSED_TIME#4189,AIR_TIME#4190,DISTANCE#4191,CARRIER_DELAY#4192,WEATHER_DELAY#4193,... 4 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "#PageRank (custom)\n",
    "# Custom iterative PageRank implementation (simplified version)\n",
    "# This is a simplified version and assumes uniform weights and damping\n",
    "\n",
    "N = airports_df.count()\n",
    "damping = 0.85\n",
    "iterations = 10\n",
    "\n",
    "ranks = airports_df.withColumn(\"rank\", F.lit(1.0 / N))\n",
    "\n",
    "for i in range(iterations):\n",
    "    contribs = edges_df.join(ranks, edges_df.src == ranks.id, \"left\") \\\n",
    "        .groupBy(\"dst\") \\\n",
    "        .agg(F.sum(F.col(\"rank\") / F.count(\"src\")).alias(\"contrib\"))\n",
    "\n",
    "    ranks = contribs.withColumn(\"rank\", (1 - damping) / N + damping * F.col(\"contrib\")) \\\n",
    "        .select(F.col(\"dst\").alias(\"id\"), \"rank\") \\\n",
    "        .union(airports_df.join(contribs, airports_df.id == contribs.dst, \"left_anti\")\n",
    "               .select(\"id\").withColumn(\"rank\", F.lit((1 - damping) / N)))\n",
    "\n",
    "display(ranks.orderBy(\"rank\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most connected airports (based on total degree)\n",
    "most_connected = degree_df.orderBy(\"totalDegree\", ascending=False).limit(10)\n",
    "display(most_connected)\n",
    "\n",
    "# Optional: visualizations with matplotlib or seaborn if you extract data locally"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Dataframe",
   "notebookOrigID": 1061204080530756,
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
