{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1w9AYcq5R1L"
      },
      "source": [
        "# Big Data Management Project 4:\n",
        "## Airline Delay and Cancellation Prediction with Spark ML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9NtdW73ROym"
      },
      "source": [
        "### Imports & Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yY93y0lROym"
      },
      "source": [
        "### 1. Data Ingestion and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Obbka1FrROym"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BDM_Project4\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yRK-kseRROym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246db69d-472f-4dac-9f30-f8a57d22b6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- FL_DATE: date (nullable = true)\n",
            " |-- OP_CARRIER: string (nullable = true)\n",
            " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
            " |-- ORIGIN: string (nullable = true)\n",
            " |-- DEST: string (nullable = true)\n",
            " |-- CRS_DEP_TIME: integer (nullable = true)\n",
            " |-- DEP_TIME: double (nullable = true)\n",
            " |-- DEP_DELAY: double (nullable = true)\n",
            " |-- TAXI_OUT: double (nullable = true)\n",
            " |-- WHEELS_OFF: double (nullable = true)\n",
            " |-- WHEELS_ON: double (nullable = true)\n",
            " |-- TAXI_IN: double (nullable = true)\n",
            " |-- CRS_ARR_TIME: integer (nullable = true)\n",
            " |-- ARR_TIME: double (nullable = true)\n",
            " |-- ARR_DELAY: double (nullable = true)\n",
            " |-- CANCELLED: double (nullable = true)\n",
            " |-- CANCELLATION_CODE: string (nullable = true)\n",
            " |-- DIVERTED: double (nullable = true)\n",
            " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
            " |-- ACTUAL_ELAPSED_TIME: double (nullable = true)\n",
            " |-- AIR_TIME: double (nullable = true)\n",
            " |-- DISTANCE: double (nullable = true)\n",
            " |-- CARRIER_DELAY: double (nullable = true)\n",
            " |-- WEATHER_DELAY: double (nullable = true)\n",
            " |-- NAS_DELAY: double (nullable = true)\n",
            " |-- SECURITY_DELAY: double (nullable = true)\n",
            " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
            " |-- Unnamed: 27: string (nullable = true)\n",
            "\n",
            "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEP_TIME|DEP_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 27|\n",
            "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
            "|2009-01-01|        XE|             1204|   DCA| EWR|        1100|  1058.0|     -2.0|    18.0|    1116.0|   1158.0|    8.0|        1202|  1206.0|      4.0|      0.0|             NULL|     0.0|            62.0|               68.0|    42.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
            "|2009-01-01|        XE|             1206|   EWR| IAD|        1510|  1509.0|     -1.0|    28.0|    1537.0|   1620.0|    4.0|        1632|  1624.0|     -8.0|      0.0|             NULL|     0.0|            82.0|               75.0|    43.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
            "|2009-01-01|        XE|             1207|   EWR| DCA|        1100|  1059.0|     -1.0|    20.0|    1119.0|   1155.0|    6.0|        1210|  1201.0|     -9.0|      0.0|             NULL|     0.0|            70.0|               62.0|    36.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
            "|2009-01-01|        XE|             1208|   DCA| EWR|        1240|  1249.0|      9.0|    10.0|    1259.0|   1336.0|    9.0|        1357|  1345.0|    -12.0|      0.0|             NULL|     0.0|            77.0|               56.0|    37.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
            "|2009-01-01|        XE|             1209|   IAD| EWR|        1715|  1705.0|    -10.0|    24.0|    1729.0|   1809.0|   13.0|        1900|  1822.0|    -38.0|      0.0|             NULL|     0.0|           105.0|               77.0|    40.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
            "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2009 = (spark.read\n",
        "      .option(\"header\", \"true\")\n",
        "      .option(\"inferSchema\", \"true\")\n",
        "      .csv(\"input/2009.csv\"))\n",
        "df2009.printSchema()\n",
        "df2009.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujO9eV9QROyn"
      },
      "source": [
        "#### Save as parquet, partitioned by extracted month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Djg6w2bNROyn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, month, dayofweek\n",
        "\n",
        "df2009 = df2009.withColumn(\"Month\", month(col(\"FL_DATE\")))\n",
        "df2009 = df2009.withColumn(\"DayOfWeek\", dayofweek(col(\"FL_DATE\")))\n",
        "df2009.write.mode(\"overwrite\").partitionBy(\"Month\").parquet(\"/home/jovyan/input/2009_parquet/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ALhnXNHROyn"
      },
      "source": [
        "### 2. Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P8bdWbcDROyn"
      },
      "outputs": [],
      "source": [
        "df2009 = df2009.drop(\"Unnamed: 27\")\n",
        "\n",
        "df2009 = df2009.withColumnRenamed(\"OP_CARRIER\", \"UniqueCarrier\")\n",
        "df2009 = df2009.withColumnRenamed(\"OP_CARRIER_FL_NUM\", \"UniqueCarrierFlightNumber\")\n",
        "\n",
        "df2009_cleaned = df2009.dropna(subset=['FL_DATE', 'UniqueCarrier'])\n",
        "\n",
        "df2009_cleaned = df2009_cleaned.filter(col(\"DIVERTED\") == 0.0)\n",
        "\n",
        "df2009_cleaned = df2009_cleaned.withColumn(\"Month\", month(\"FL_DATE\")) \\\n",
        "                                 .withColumn(\"DayOfWeek\", dayofweek(\"FL_DATE\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWVxxYGOROyn"
      },
      "source": [
        "### 3. Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2009_cleaned.groupBy(\"CANCELLED\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rtan-2PgZLN",
        "outputId": "a2124a4b-1a64-4f01-a56c-dc3d474ddc28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|CANCELLED| count|\n",
            "+---------+------+\n",
            "|      0.0|123691|\n",
            "|      1.0|  2183|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M7AdouUROyn",
        "outputId": "8e216197-c188-4a6e-d36e-8a99cc8d941c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2183"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "cancelled = df2009_cleaned.filter(df2009_cleaned[\"CANCELLED\"] == 1.0)\n",
        "cancelled.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwh0qIzRROyn",
        "outputId": "e85e7894-257a-4fb0-c958-b648a97352df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|UniqueCarrier|count|\n",
            "+-------------+-----+\n",
            "|           WN|  295|\n",
            "|           OO|  252|\n",
            "|           MQ|  213|\n",
            "|           EV|  202|\n",
            "|           AA|  192|\n",
            "|           US|  147|\n",
            "|           9E|  147|\n",
            "|           UA|  113|\n",
            "|           DL|  100|\n",
            "|           XE|   99|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Top 10 carriers by number of flights\n",
        "top_10_carriers = cancelled.groupBy(\"UniqueCarrier\").count().orderBy(\"count\", ascending=False).limit(10)\n",
        "top_10_carriers.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWZhg9TkROyn",
        "outputId": "346af90f-29f1-4dea-8e28-ae17d393f6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|CANCELLATION_CODE|count|\n",
            "+-----------------+-----+\n",
            "|                A|  933|\n",
            "|                B|  862|\n",
            "|                C|  388|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count cancellation reasons\n",
        "cancel_reasons = cancelled.groupBy(\"CANCELLATION_CODE\").count().orderBy(\"count\", ascending=False)\n",
        "cancel_reasons.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iwUvzfTROyo"
      },
      "source": [
        "From Kaggle:\n",
        "Reason for Cancellation of flight:\n",
        "A - Airline/Carrier;\n",
        "B - Weather;\n",
        "C - National Air System;\n",
        "D - Security"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "pqYN3A_oROyo",
        "outputId": "9656e61d-a6ad-4b25-eb42-78e4997d209f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "cancel_reasons_pd = cancel_reasons.toPandas()\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.bar(cancel_reasons_pd[\"CANCELLATION_CODE\"], cancel_reasons_pd[\"count\"], color='salmon')\n",
        "plt.xlabel(\"Cancellation Code\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Cancellation Reasons\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufm6SlucROyo",
        "outputId": "c4457111-1fd6-4da2-b1fe-e401d10a4af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|   Route|count|\n",
            "+--------+-----+\n",
            "|OGG->HNL|  252|\n",
            "|HNL->OGG|  245|\n",
            "|SFO->LAX|  238|\n",
            "|LAX->SFO|  229|\n",
            "|HNL->LIH|  209|\n",
            "|LGA->ORD|  208|\n",
            "|ORD->LGA|  203|\n",
            "|LIH->HNL|  202|\n",
            "|LAS->LAX|  199|\n",
            "|SAN->LAX|  199|\n",
            "+--------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "df2009_cleaned.withColumn(\"Route\", concat_ws(\"->\", \"ORIGIN\", \"DEST\")) \\\n",
        "               .groupBy(\"Route\").count().orderBy(\"count\", ascending=False).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTneOz1tROyo",
        "outputId": "59b76087-9119-4945-8b21-ec539facdc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|CANCELLED| count|\n",
            "+---------+------+\n",
            "|      0.0|123691|\n",
            "|      1.0|  2183|\n",
            "+---------+------+\n",
            "\n",
            "Imbalance Ratio: 56.66\n"
          ]
        }
      ],
      "source": [
        "class_dist = df2009_cleaned.groupBy(\"CANCELLED\").count()\n",
        "class_dist.show()\n",
        "\n",
        "counts = class_dist.collect()\n",
        "class_counts = {row['CANCELLED']: row['count'] for row in counts}\n",
        "\n",
        "imbalance_ratio = max(class_counts.values()) / min(class_counts.values())\n",
        "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ipUJ4RROyo"
      },
      "outputs": [],
      "source": [
        "#4. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN14P5a0ROyo"
      },
      "outputs": [],
      "source": [
        "#Categorical features (UniqueCarrier, ORIGIN, DEST) were encoded using StringIndexer and OneHotEncoder. Numerical features (DEP_DELAY, ARR_DELAY, CRS_ELAPSED_TIME) were included directly. All features were combined into a single vector with VectorAssembler for use in modeling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4. Feature Engineering === Eidi\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cat_cols = [\"UniqueCarrier\", \"ORIGIN\", \"DEST\"]\n",
        "# DEP_DELAY ja ARR_DELAY jätsin välja, kuna need veerud sisaldavad\n",
        "#tühistatud lendude (CANCELLED = 1.0) puhul puuduvaid väärtusi.\n",
        "# Kui need jätta sisse, siis VectorAssembler\n",
        "# df_prepared_clean = df_prepared.dropna(subset=assembler_inputs + [\"CANCELLED\"])\n",
        "#viskab need read välja (Eemalab kõik read, kus mõni\n",
        "#VectorAssembler sisend või 'CANCELLED' on null.\"\n",
        "# Mudel ei saa õppida tühistatud lendude pealt.\n",
        "# Kasutasin ainult CRS_ELAPSED_TIME, mis on kõigi lendude puhul olemas.\n",
        "# ehk siis tehtniliselt, siia peab vaatama veerunimesid mille puhul\n",
        "# tunnused oleks kõigi lendue puhul ka olemas, mitte null valuga\n",
        "num_cols = [\"CRS_ELAPSED_TIME\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\"]\n",
        "\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"keep\") for col in cat_cols]\n",
        "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_Vec\", handleInvalid=\"keep\") for col in cat_cols]\n",
        "\n",
        "assembler_inputs = [col + \"_Vec\" for col in cat_cols] + num_cols\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "\n",
        "pipeline_model = pipeline.fit(df2009_cleaned)\n",
        "df_prepared = pipeline_model.transform(df2009_cleaned)\n",
        "\n",
        "df_prepared_clean = df_prepared.dropna(subset=assembler_inputs + [\"CANCELLED\"])\n",
        "\n",
        "df_prepared_clean.select(\"features\", \"CANCELLED\").show(5, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_-zqlXhbsoL",
        "outputId": "4c966bc7-8fa7-4a40-8c55-5895b8d486e8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------+---------+\n",
            "|features                                                      |CANCELLED|\n",
            "+--------------------------------------------------------------+---------+\n",
            "|(581,[9,42,311,578,579,580],[1.0,1.0,1.0,62.0,1100.0,1202.0]) |0.0      |\n",
            "|(581,[9,32,327,578,579,580],[1.0,1.0,1.0,82.0,1510.0,1632.0]) |0.0      |\n",
            "|(581,[9,32,321,578,579,580],[1.0,1.0,1.0,70.0,1100.0,1210.0]) |0.0      |\n",
            "|(581,[9,42,311,578,579,580],[1.0,1.0,1.0,77.0,1240.0,1357.0]) |0.0      |\n",
            "|(581,[9,49,311,578,579,580],[1.0,1.0,1.0,105.0,1715.0,1900.0])|0.0      |\n",
            "+--------------------------------------------------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2009_cleaned.filter(\"CANCELLED = 1.0\").select(\"CRS_ELAPSED_TIME\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\").summary(\"count\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_TXRcvzoKEC",
        "outputId": "aa172d63-3b1c-4e34-9065-fddbd4423d3d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+------------+------------+\n",
            "|summary|CRS_ELAPSED_TIME|CRS_DEP_TIME|CRS_ARR_TIME|\n",
            "+-------+----------------+------------+------------+\n",
            "|  count|            2183|        2183|        2183|\n",
            "+-------+----------------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prepared.filter(\"CANCELLED = 1.0\").select(*assembler_inputs).summary(\"count\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffcintn5nhD6",
        "outputId": "d2f713b2-fc0f-4d31-f877-cbf0eca92414"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+------------+------------+\n",
            "|summary|CRS_ELAPSED_TIME|CRS_DEP_TIME|CRS_ARR_TIME|\n",
            "+-------+----------------+------------+------------+\n",
            "|  count|            2183|        2183|        2183|\n",
            "+-------+----------------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prepared_clean.groupBy(\"CANCELLED\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fOEwBXJkeH7",
        "outputId": "7a295f83-23e1-4678-9824-215e915c99a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|CANCELLED| count|\n",
            "+---------+------+\n",
            "|      0.0|123691|\n",
            "|      1.0|  2183|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghLrhZKIROyo",
        "outputId": "2fca5430-fa87-405e-fb82-4788390ebe5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------+---------+\n",
            "|features                                                    |CANCELLED|\n",
            "+------------------------------------------------------------+---------+\n",
            "|(575,[9,40,307,572,573,574],[1.0,1.0,1.0,-2.0,4.0,62.0])    |0.0      |\n",
            "|(575,[9,30,323,572,573,574],[1.0,1.0,1.0,-1.0,-8.0,82.0])   |0.0      |\n",
            "|(575,[9,30,317,572,573,574],[1.0,1.0,1.0,-1.0,-9.0,70.0])   |0.0      |\n",
            "|(575,[9,40,307,572,573,574],[1.0,1.0,1.0,9.0,-12.0,77.0])   |0.0      |\n",
            "|(575,[9,47,307,572,573,574],[1.0,1.0,1.0,-10.0,-38.0,105.0])|0.0      |\n",
            "+------------------------------------------------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cat_cols = [\"UniqueCarrier\", \"ORIGIN\", \"DEST\"]\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\") for col in cat_cols]\n",
        "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_Vec\") for col in cat_cols]\n",
        "\n",
        "num_cols = [\"DEP_DELAY\", \"ARR_DELAY\", \"CRS_ELAPSED_TIME\"]\n",
        "\n",
        "assembler_inputs = [col + \"_Vec\" for col in cat_cols] + num_cols\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "pipeline_model = pipeline.fit(df2009_cleaned)\n",
        "df_prepared = pipeline_model.transform(df2009_cleaned)\n",
        "\n",
        "df_prepared.select(\"features\", \"CANCELLED\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOZh_a9zROyo",
        "outputId": "6287cea0-9a09-4e5d-f911-fb428a413e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------------------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----+---------+-------------------+------------+----------+-----------------+----------------+----------------+--------------------+\n",
            "|   FL_DATE|UniqueCarrier|UniqueCarrierFlightNumber|ORIGIN|DEST|CRS_DEP_TIME|DEP_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Month|DayOfWeek|UniqueCarrier_Index|ORIGIN_Index|DEST_Index|UniqueCarrier_Vec|      ORIGIN_Vec|        DEST_Vec|            features|\n",
            "+----------+-------------+-------------------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----+---------+-------------------+------------+----------+-----------------+----------------+----------------+--------------------+\n",
            "|2009-01-01|           XE|                     1204|   DCA| EWR|        1100|  1058.0|     -2.0|    18.0|    1116.0|   1158.0|    8.0|        1202|  1206.0|      4.0|      0.0|             NULL|     0.0|            62.0|               68.0|    42.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        22.0|      12.0|   (20,[9],[1.0])|(279,[22],[1.0])|(279,[12],[1.0])|(581,[9,42,311,57...|\n",
            "|2009-01-01|           XE|                     1206|   EWR| IAD|        1510|  1509.0|     -1.0|    28.0|    1537.0|   1620.0|    4.0|        1632|  1624.0|     -8.0|      0.0|             NULL|     0.0|            82.0|               75.0|    43.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        12.0|      28.0|   (20,[9],[1.0])|(279,[12],[1.0])|(279,[28],[1.0])|(581,[9,32,327,57...|\n",
            "|2009-01-01|           XE|                     1207|   EWR| DCA|        1100|  1059.0|     -1.0|    20.0|    1119.0|   1155.0|    6.0|        1210|  1201.0|     -9.0|      0.0|             NULL|     0.0|            70.0|               62.0|    36.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        12.0|      22.0|   (20,[9],[1.0])|(279,[12],[1.0])|(279,[22],[1.0])|(581,[9,32,321,57...|\n",
            "|2009-01-01|           XE|                     1208|   DCA| EWR|        1240|  1249.0|      9.0|    10.0|    1259.0|   1336.0|    9.0|        1357|  1345.0|    -12.0|      0.0|             NULL|     0.0|            77.0|               56.0|    37.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        22.0|      12.0|   (20,[9],[1.0])|(279,[22],[1.0])|(279,[12],[1.0])|(581,[9,42,311,57...|\n",
            "|2009-01-01|           XE|                     1209|   IAD| EWR|        1715|  1705.0|    -10.0|    24.0|    1729.0|   1809.0|   13.0|        1900|  1822.0|    -38.0|      0.0|             NULL|     0.0|           105.0|               77.0|    40.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        29.0|      12.0|   (20,[9],[1.0])|(279,[29],[1.0])|(279,[12],[1.0])|(581,[9,49,311,57...|\n",
            "|2009-01-01|           XE|                     1212|   ATL| EWR|        1915|  1913.0|     -2.0|    19.0|    1932.0|   2108.0|   15.0|        2142|  2123.0|    -19.0|      0.0|             NULL|     0.0|           147.0|              130.0|    96.0|   745.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|         0.0|      12.0|   (20,[9],[1.0])| (279,[0],[1.0])|(279,[12],[1.0])|(581,[9,20,311,57...|\n",
            "|2009-01-01|           XE|                     1212|   CLE| ATL|        1645|  1637.0|     -8.0|    12.0|    1649.0|   1820.0|    5.0|        1842|  1825.0|    -17.0|      0.0|             NULL|     0.0|           117.0|              108.0|    91.0|   554.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        33.0|       0.0|   (20,[9],[1.0])|(279,[33],[1.0])| (279,[0],[1.0])|(581,[9,53,299,57...|\n",
            "|2009-01-01|           XE|                     1214|   DCA| EWR|        1915|  1908.0|     -7.0|     9.0|    1917.0|   1953.0|   34.0|        2035|  2027.0|     -8.0|      0.0|             NULL|     0.0|            80.0|               79.0|    36.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        22.0|      12.0|   (20,[9],[1.0])|(279,[22],[1.0])|(279,[12],[1.0])|(581,[9,42,311,57...|\n",
            "|2009-01-01|           XE|                     1215|   EWR| DCA|        1715|  1710.0|     -5.0|    28.0|    1738.0|   1819.0|    4.0|        1838|  1823.0|    -15.0|      0.0|             NULL|     0.0|            83.0|               73.0|    41.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        12.0|      22.0|   (20,[9],[1.0])|(279,[12],[1.0])|(279,[22],[1.0])|(581,[9,32,321,57...|\n",
            "|2009-01-01|           XE|                     1217|   EWR| DCA|        1300|  1255.0|     -5.0|    15.0|    1310.0|   1349.0|    7.0|        1408|  1356.0|    -12.0|      0.0|             NULL|     0.0|            68.0|               61.0|    39.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        12.0|      22.0|   (20,[9],[1.0])|(279,[12],[1.0])|(279,[22],[1.0])|(581,[9,32,321,57...|\n",
            "|2009-01-01|           XE|                     1218|   DCA| EWR|        1500|  1457.0|     -3.0|    14.0|    1511.0|   1552.0|    7.0|        1620|  1559.0|    -21.0|      0.0|             NULL|     0.0|            80.0|               62.0|    41.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        22.0|      12.0|   (20,[9],[1.0])|(279,[22],[1.0])|(279,[12],[1.0])|(581,[9,42,311,57...|\n",
            "|2009-01-01|           XE|                     1219|   EWR| DCA|        2135|  2131.0|     -4.0|    21.0|    2152.0|   2232.0|    3.0|        2252|  2235.0|    -17.0|      0.0|             NULL|     0.0|            77.0|               64.0|    40.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        12.0|      22.0|   (20,[9],[1.0])|(279,[12],[1.0])|(279,[22],[1.0])|(581,[9,32,321,57...|\n",
            "|2009-01-01|           XE|                     1220|   CLE| DCA|        1905|  1855.0|    -10.0|    10.0|    1905.0|   1956.0|    5.0|        2025|  2001.0|    -24.0|      0.0|             NULL|     0.0|            80.0|               66.0|    51.0|   310.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        33.0|      22.0|   (20,[9],[1.0])|(279,[33],[1.0])|(279,[22],[1.0])|(581,[9,53,321,57...|\n",
            "|2009-01-01|           XE|                     1220|   DCA| EWR|        2100|  2049.0|    -11.0|    10.0|    2059.0|   2133.0|   10.0|        2217|  2143.0|    -34.0|      0.0|             NULL|     0.0|            77.0|               54.0|    34.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        22.0|      12.0|   (20,[9],[1.0])|(279,[22],[1.0])|(279,[12],[1.0])|(581,[9,42,311,57...|\n",
            "|2009-01-01|           XE|                     1232|   ORD| EWR|         905|   900.0|     -5.0|    16.0|     916.0|   1144.0|    6.0|        1212|  1150.0|    -22.0|      0.0|             NULL|     0.0|           127.0|              110.0|    88.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|         1.0|      12.0|   (20,[9],[1.0])| (279,[1],[1.0])|(279,[12],[1.0])|(581,[9,21,311,57...|\n",
            "|2009-01-01|           XE|                     1233|   EWR| ORD|        1000|  1035.0|     35.0|    14.0|    1049.0|   1156.0|   10.0|        1139|  1206.0|     27.0|      0.0|             NULL|     0.0|           159.0|              151.0|   127.0|   719.0|          0.0|         27.0|      0.0|           0.0|                0.0|    1|        5|                9.0|        12.0|       1.0|   (20,[9],[1.0])|(279,[12],[1.0])| (279,[1],[1.0])|(581,[9,32,300,57...|\n",
            "|2009-01-01|           XE|                     1234|   ORD| EWR|        1230|  1234.0|      4.0|     8.0|    1242.0|   1511.0|   14.0|        1559|  1525.0|    -34.0|      0.0|             NULL|     0.0|           149.0|              111.0|    89.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|         1.0|      12.0|   (20,[9],[1.0])| (279,[1],[1.0])|(279,[12],[1.0])|(581,[9,21,311,57...|\n",
            "|2009-01-01|           XE|                     1235|   EWR| ORD|        1343|  1406.0|     23.0|    13.0|    1419.0|   1523.0|    7.0|        1523|  1530.0|      7.0|      0.0|             NULL|     0.0|           160.0|              144.0|   124.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        12.0|       1.0|   (20,[9],[1.0])|(279,[12],[1.0])| (279,[1],[1.0])|(581,[9,32,300,57...|\n",
            "|2009-01-01|           XE|                     1236|   ORD| EWR|        1630|  1619.0|    -11.0|    19.0|    1638.0|   1906.0|   35.0|        2002|  1941.0|    -21.0|      0.0|             NULL|     0.0|           152.0|              142.0|    88.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|         1.0|      12.0|   (20,[9],[1.0])| (279,[1],[1.0])|(279,[12],[1.0])|(581,[9,21,311,57...|\n",
            "|2009-01-01|           XE|                     1237|   EWR| ORD|        1930|  1927.0|     -3.0|    16.0|    1943.0|   2049.0|    8.0|        2123|  2057.0|    -26.0|      0.0|             NULL|     0.0|           173.0|              150.0|   126.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|    1|        5|                9.0|        12.0|       1.0|   (20,[9],[1.0])|(279,[12],[1.0])| (279,[1],[1.0])|(581,[9,32,300,57...|\n",
            "+----------+-------------+-------------------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----+---------+-------------------+------------+----------+-----------------+----------------+----------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_prepared.describe()\n",
        "df_prepared.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnTXanSZROyo"
      },
      "outputs": [],
      "source": [
        "#5. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 modeling eidi\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = df_prepared.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "#defining models\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Gradient-Boosted Tree Classifier\n",
        "gbt = GBTClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")"
      ],
      "metadata": {
        "id": "faRQMkuZqiI1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.evaluators eidi\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"CANCELLED\", metricName=\"areaUnderROC\")\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"CANCELLED\", metricName=\"accuracy\")"
      ],
      "metadata": {
        "id": "LUgwDIN6qnqr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define parameter grids eidi\n",
        "lr_grid = ParamGridBuilder().addGrid(lr.maxIter, [10, 50]).build()\n",
        "dt_grid = ParamGridBuilder().addGrid(dt.maxDepth, [5, 10]).build()\n",
        "rf_grid = ParamGridBuilder().addGrid(rf.numTrees, [10, 50]).build()\n",
        "gbt_grid = ParamGridBuilder().addGrid(gbt.maxIter, [10, 50]).build()"
      ],
      "metadata": {
        "id": "OCJIPaz3quAL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. CrossValidators eidi\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=lr_grid, evaluator=evaluator_auc, numFolds=3)\n",
        "cv_dt = CrossValidator(estimator=dt, estimatorParamMaps=dt_grid, evaluator=evaluator_auc, numFolds=3)\n",
        "cv_rf = CrossValidator(estimator=rf, estimatorParamMaps=rf_grid, evaluator=evaluator_auc, numFolds=3)\n",
        "cv_gbt = CrossValidator(estimator=gbt, estimatorParamMaps=gbt_grid, evaluator=evaluator_auc, numFolds=3)"
      ],
      "metadata": {
        "id": "rpmM6-M0q2tp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train models eidi\n",
        "lr_model = cv_lr.fit(train_data)\n",
        "dt_model = cv_dt.fit(train_data)\n",
        "rf_model = cv_rf.fit(train_data)\n",
        "gbt_model = cv_gbt.fit(train_data)"
      ],
      "metadata": {
        "id": "MHmoGI_gq6cF"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. predictions eidi\n",
        "lr_pred = lr_model.transform(test_data)\n",
        "dt_pred = dt_model.transform(test_data)\n",
        "rf_pred = rf_model.transform(test_data)\n",
        "gbt_pred = gbt_model.transform(test_data)\n"
      ],
      "metadata": {
        "id": "Kfnat34Xq_Gu"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Evaluate eidi\n",
        "results = []\n",
        "\n",
        "def evaluate_model(name, predictions):\n",
        "    accuracy = evaluator_acc.evaluate(predictions)\n",
        "    auc = evaluator_auc.evaluate(predictions)\n",
        "    results.append({\"Model\": name, \"Accuracy\": accuracy, \"AUC\": auc})\n",
        "    print(f\"{name} — Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", lr_pred)\n",
        "evaluate_model(\"Decision Tree\", dt_pred)\n",
        "evaluate_model(\"Random Forest\", rf_pred)\n",
        "evaluate_model(\"Gradient Boosted Trees\", gbt_pred)\n",
        "\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqBg_X9ludkE",
        "outputId": "94c9e4b0-7af9-47e7-cbd4-7776025c04eb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression — Accuracy: 0.9824, AUC: 0.7150\n",
            "Decision Tree — Accuracy: 0.9820, AUC: 0.5462\n",
            "Random Forest — Accuracy: 0.9825, AUC: 0.6572\n",
            "Gradient Boosted Trees — Accuracy: 0.9822, AUC: 0.6919\n",
            "                    Model  Accuracy       AUC\n",
            "0     Logistic Regression  0.982419  0.714982\n",
            "1           Decision Tree  0.982048  0.546160\n",
            "2           Random Forest  0.982471  0.657195\n",
            "3  Gradient Boosted Trees  0.982233  0.691870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PART 6 EIDI"
      ],
      "metadata": {
        "id": "FP0ACa3k5pqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUC mõhjal on gbt_model.bestModel parem\n",
        "best_tree_model = gbt_model.bestModel  # või rf_model.bestModel\n",
        "\n",
        "importances = best_tree_model.featureImportances\n"
      ],
      "metadata": {
        "id": "10xLI-ve5swV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = assembler.getInputCols()\n"
      ],
      "metadata": {
        "id": "13AO4feO53xx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_importance_list = list(zip(feature_names, importances))\n",
        "\n",
        "sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n"
      ],
      "metadata": {
        "id": "dqaAK6Pa560_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_k = 5\n",
        "top_features = sorted_features[:top_k]\n",
        "\n",
        "names = [f[0] for f in top_features]\n",
        "scores = [f[1] for f in top_features]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(names[::-1], scores[::-1])\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"Top Feature Importances (Tree-based Model)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "RP12Rv8X59ot",
        "outputId": "55cbf979-8104-4e71-e24f-8dcc39b64223"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U94CfYtFROyo"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = df_prepared.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Gradient-Boosted Tree Classifier\n",
        "gbt = GBTClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwMtM1goROyo"
      },
      "outputs": [],
      "source": [
        "# Evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\")\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_model(model, train_data, test_data):\n",
        "    # Fit the model\n",
        "    model_fitted = model.fit(train_data)\n",
        "\n",
        "    # Predictions on test data\n",
        "    predictions = model_fitted.transform(test_data)\n",
        "\n",
        "    # AUC and Accuracy\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "    accuracy = predictions.filter(predictions[\"CANCELLED\"] == predictions[\"prediction\"]).count() / float(predictions.count())\n",
        "\n",
        "    return accuracy, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0XjT-PqROyo"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Define the cross-validation evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\")\n",
        "\n",
        "# Logistic Regression Hyperparameter Tuning\n",
        "lr_param_grid = (ParamGridBuilder()\n",
        "                 .addGrid(lr.maxIter, [10, 50, 100])\n",
        "                 .build())\n",
        "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_param_grid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# Decision Tree Hyperparameter Tuning\n",
        "dt_param_grid = (ParamGridBuilder()\n",
        "                 .addGrid(dt.maxDepth, [5, 10, 15])\n",
        "                 .build())\n",
        "dt_cv = CrossValidator(estimator=dt, estimatorParamMaps=dt_param_grid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# Random Forest Hyperparameter Tuning\n",
        "rf_param_grid = (ParamGridBuilder()\n",
        "                 .addGrid(rf.numTrees, [10, 20, 50])\n",
        "                 .build())\n",
        "rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_param_grid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# GBT Hyperparameter Tuning\n",
        "gbt_param_grid = (ParamGridBuilder()\n",
        "                  .addGrid(gbt.maxIter, [10, 50, 100])\n",
        "                  .build())\n",
        "gbt_cv = CrossValidator(estimator=gbt, estimatorParamMaps=gbt_param_grid, evaluator=evaluator, numFolds=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wa2sXN4KROyo",
        "outputId": "7047c4fb-e19f-4f60-d819-003693f0beeb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1865.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 77.0 failed 1 times, most recent failure: Lost task 1.0 in stage 77.0 (TID 80) (617e888bee8d executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4372/0x00000008418db040`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4372/0x00000008418db040`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c7c638e66987>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train models with cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgbt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             )\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mmetrics_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             )\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mmetrics_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/util.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLocalProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#  Note: Supporting tuning params in evaluator need update method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No models remaining.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1865.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 77.0 failed 1 times, most recent failure: Lost task 1.0 in stage 77.0 (TID 80) (617e888bee8d executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4372/0x00000008418db040`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4372/0x00000008418db040`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n"
          ]
        }
      ],
      "source": [
        "# Train models with cross-validation\n",
        "lr_model = lr_cv.fit(train_data)\n",
        "dt_model = dt_cv.fit(train_data)\n",
        "rf_model = rf_cv.fit(train_data)\n",
        "gbt_model = gbt_cv.fit(train_data)\n",
        "\n",
        "# Evaluate models\n",
        "lr_accuracy, lr_auc = evaluate_model(lr_model.bestModel, train_data, test_data)\n",
        "dt_accuracy, dt_auc = evaluate_model(dt_model.bestModel, train_data, test_data)\n",
        "rf_accuracy, rf_auc = evaluate_model(rf_model.bestModel, train_data, test_data)\n",
        "gbt_accuracy, gbt_auc = evaluate_model(gbt_model.bestModel, train_data, test_data)\n",
        "\n",
        "# Print results\n",
        "print(f\"Logistic Regression - Accuracy: {lr_accuracy}, AUC: {lr_auc}\")\n",
        "print(f\"Decision Tree - Accuracy: {dt_accuracy}, AUC: {dt_auc}\")\n",
        "print(f\"Random Forest - Accuracy: {rf_accuracy}, AUC: {rf_auc}\")\n",
        "print(f\"GBT - Accuracy: {gbt_accuracy}, AUC: {gbt_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4nIL-UEROyo"
      },
      "outputs": [],
      "source": [
        "#proovin eraldi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x472kp-JROyo"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model using the test data\n",
        "def evaluate_model(model, test_data):\n",
        "    # Transform test data using the trained model\n",
        "    test_data_transformed = model.transform(test_data)\n",
        "    auc = evaluator.evaluate(test_data_transformed)\n",
        "    print(f\"AUC: {auc}\")\n",
        "    return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "vk-qh8-7ROyp",
        "outputId": "87999b4c-d18c-43bb-833e-729dc0a733ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Output column features already exists.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d7d9e7a99c50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 4. Create VectorAssembler to combine the features into a single vector column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandleInvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdf_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# 5. Define the Logistic Regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Output column features already exists."
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming you already have df_prepared as your DataFrame\n",
        "\n",
        "# 1. Handle String columns with StringIndexer (convert categorical columns to numerical)\n",
        "categorical_columns = ['UniqueCarrier', 'ORIGIN', 'DEST', 'CANCELLATION_CODE']  # Add your categorical columns here\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_columns]\n",
        "\n",
        "# Apply StringIndexers to convert categorical columns to numerical\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df_copy = pipeline.fit(df_prepared).transform(df_prepared)\n",
        "\n",
        "# 2. Handle Date columns by extracting year, month, etc.\n",
        "date_columns = ['FL_DATE']  # Replace with your actual date columns\n",
        "for date_col in date_columns:\n",
        "    # Extract year, month, and day from the date column\n",
        "    df_copy = df_copy.withColumn(f\"{date_col}_year\", F.year(F.col(date_col)))\n",
        "    df_copy = df_copy.withColumn(f\"{date_col}_month\", F.month(F.col(date_col)))\n",
        "    df_copy = df_copy.withColumn(f\"{date_col}_day\", F.dayofmonth(F.col(date_col)))\n",
        "\n",
        "# 3. Prepare the features by selecting relevant columns (dropping 'label' column)\n",
        "column_names = df_copy.columns\n",
        "df_copy = df_copy.dropna(subset=column_names)\n",
        "\n",
        "# Assuming 'CANCELLED' is your target column, drop it from features\n",
        "feature_columns = [col for col in df_copy.columns if col not in ['CANCELLED'] + categorical_columns + date_columns]\n",
        "\n",
        "# 4. Create VectorAssembler to combine the features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "df_copy = assembler.transform(df_copy)\n",
        "\n",
        "# 5. Define the Logistic Regression model\n",
        "lr = LogisticRegression(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Hyperparameter grid for cross-validation\n",
        "lr_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Binary Classification Evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\")\n",
        "\n",
        "# Cross-validation setup\n",
        "lr_cv = CrossValidator(estimator=lr,\n",
        "                       estimatorParamMaps=lr_param_grid,\n",
        "                       evaluator=evaluator,\n",
        "                       numFolds=3)\n",
        "\n",
        "# Train the model with cross-validation\n",
        "lr_model = lr_cv.fit(df_copy)\n",
        "\n",
        "# 6. Evaluate the model using the test data\n",
        "def evaluate_model(model, test_data):\n",
        "    # Transform test data using the trained model\n",
        "    test_data_transformed = model.transform(test_data)\n",
        "    auc = evaluator.evaluate(test_data_transformed)\n",
        "    print(f\"AUC: {auc}\")\n",
        "    return auc\n",
        "\n",
        "# Split the data into train and test datasets (ensure it is done after feature engineering)\n",
        "train_data, test_data = df_copy.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(lr_model, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVF3XCp9ROyp",
        "outputId": "65ff8646-1156-4c4f-f5c1-af837d1e954d"
      },
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "Data type date of column FL_DATE is not supported.\nData type string of column UniqueCarrier is not supported.\nData type string of column ORIGIN is not supported.\nData type string of column DEST is not supported.\nData type string of column CANCELLATION_CODE is not supported.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create VectorAssembler to combine the features into a single vector column\u001b[39;00m\n\u001b[1;32m     25\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39mfeature_columns, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m \u001b[43massembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Define the Logistic Regression model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANCELLED\u001b[39m\u001b[38;5;124m\"\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type date of column FL_DATE is not supported.\nData type string of column UniqueCarrier is not supported.\nData type string of column ORIGIN is not supported.\nData type string of column DEST is not supported.\nData type string of column CANCELLATION_CODE is not supported."
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming you already have a Spark session\n",
        "# spark = SparkSession.builder.appName(\"LogisticRegressionCV\").getOrCreate()\n",
        "\n",
        "# Assuming you already have df (DataFrame) loaded with your dataset\n",
        "\n",
        "# Create df_prepared by removing null values in any of the relevant columns\n",
        "# Here we assume df contains features and the target label column 'label'\n",
        "column_names = df_prepared.columns\n",
        "\n",
        "df_copy = df_prepared.select(\"*\")\n",
        "\n",
        "df_copy = df_copy.dropna(subset=column_names)\n",
        "train_data, test_data = df_copy.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# List of feature columns dynamically\n",
        "feature_columns = [col for col in df_copy.columns if col != 'CANCELLED']  # Assuming 'CANCELLED' is the target column\n",
        "\n",
        "# Create VectorAssembler to combine the features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "df_copy = assembler.transform(df_copy)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "lr = LogisticRegression(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Hyperparameter grid for cross-validation\n",
        "lr_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Binary Classification Evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\")\n",
        "\n",
        "# Cross-validation setup\n",
        "lr_cv = CrossValidator(estimator=lr,\n",
        "                       estimatorParamMaps=lr_param_grid,\n",
        "                       evaluator=evaluator,\n",
        "                       numFolds=3)\n",
        "\n",
        "# Train the model with cross-validation\n",
        "lr_model = lr_cv.fit(df_copy)\n",
        "\n",
        "# Evaluate the model using the test data\n",
        "def evaluate_model(model, test_data):\n",
        "    # Transform test data using the trained model\n",
        "    test_data_transformed = model.transform(test_data)\n",
        "    auc = evaluator.evaluate(test_data_transformed)\n",
        "    print(f\"AUC: {auc}\")\n",
        "    return auc\n",
        "\n",
        "# Assuming test_data is your test DataFrame, which should have the same columns as df_prepared\n",
        "evaluate_model(lr_model, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP8kDXjNROyp",
        "outputId": "91b2708f-4657-4c3d-887e-25930413a016"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o4210.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 83.0 failed 1 times, most recent failure: Lost task 1.0 in stage 83.0 (TID 665) (23b0cefcebcd executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m string_columns:\n\u001b[1;32m     27\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m StringIndexer(inputCol\u001b[38;5;241m=\u001b[39mcol, outputCol\u001b[38;5;241m=\u001b[39mcol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     df_copy \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_copy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df_copy)\n\u001b[1;32m     29\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(inputCol\u001b[38;5;241m=\u001b[39mcol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39mcol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_onehot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     df_copy \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(df_copy)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4210.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 83.0 failed 1 times, most recent failure: Lost task 1.0 in stage 83.0 (TID 665) (23b0cefcebcd executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming you already have a Spark session\n",
        "# spark = SparkSession.builder.appName(\"LogisticRegressionCV\").getOrCreate()\n",
        "\n",
        "# Assuming df_prepared is already loaded with your dataset\n",
        "\n",
        "# Create df_prepared by removing null values in any of the relevant columns\n",
        "column_names = df_prepared.columns\n",
        "df_copy = df_prepared.select(\"*\")\n",
        "df_copy = df_copy.dropna(subset=column_names)\n",
        "\n",
        "# Handle date column (extract useful features like year, month, day)\n",
        "df_copy = df_copy.withColumn(\"FL_YEAR\", F.year(\"FL_DATE\")) \\\n",
        "                 .withColumn(\"FL_MONTH\", F.month(\"FL_DATE\")) \\\n",
        "                 .withColumn(\"FL_DAY\", F.dayofmonth(\"FL_DATE\"))\n",
        "\n",
        "# Handle categorical columns (e.g., UniqueCarrier, ORIGIN, DEST, CANCELLATION_CODE)\n",
        "string_columns = ['UniqueCarrier', 'ORIGIN', 'DEST', 'CANCELLATION_CODE']\n",
        "\n",
        "# Apply StringIndexer or OneHotEncoder to categorical columns\n",
        "for col in string_columns:\n",
        "    indexer = StringIndexer(inputCol=col, outputCol=col + \"_index\")\n",
        "    df_copy = indexer.fit(df_copy).transform(df_copy)\n",
        "    encoder = OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_onehot\")\n",
        "    df_copy = encoder.transform(df_copy)\n",
        "\n",
        "# List of all feature columns (excluding the target 'CANCELLED')\n",
        "feature_columns = [col for col in df_copy.columns if col not in ['CANCELLED', 'FL_DATE']]\n",
        "\n",
        "# Create VectorAssembler to combine the features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "# Transform the df_copy and overwrite the features column\n",
        "df_copy = assembler.transform(df_copy)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "lr = LogisticRegression(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Hyperparameter grid for cross-validation\n",
        "lr_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Binary Classification Evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\")\n",
        "\n",
        "# Cross-validation setup\n",
        "lr_cv = CrossValidator(estimator=lr,\n",
        "                       estimatorParamMaps=lr_param_grid,\n",
        "                       evaluator=evaluator,\n",
        "                       numFolds=3)\n",
        "\n",
        "# Train the model with cross-validation\n",
        "lr_model = lr_cv.fit(df_copy)\n",
        "\n",
        "# Evaluate the model using the test data\n",
        "def evaluate_model(model, test_data):\n",
        "    # Transform test data using the trained model\n",
        "    test_data_transformed = model.transform(test_data)\n",
        "    auc = evaluator.evaluate(test_data_transformed)\n",
        "    print(f\"AUC: {auc}\")\n",
        "    return auc\n",
        "\n",
        "# Assuming test_data is your test DataFrame, which should have the same columns as df_copy\n",
        "evaluate_model(lr_model, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRkzAWCiROyp"
      },
      "outputs": [],
      "source": [
        "columns_to_check = train_data.columns  # Replace with your actual column names\n",
        "\n",
        "for col_name in columns_to_check:\n",
        "    null_count = train_data.select(col_name).where(f\"{col_name} IS NULL\").count()\n",
        "    print(f\"Nulls in {col_name}: {null_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD7ZLAtPROyp",
        "outputId": "e32d008c-d6c9-429c-9098-1fafeb5d09d4"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o3286.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 74.0 failed 1 times, most recent failure: Lost task 8.0 in stage 74.0 (TID 546) (23b0cefcebcd executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m cv_gbt \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mgbt, estimatorParamMaps\u001b[38;5;241m=\u001b[39mgbt_param_grid, evaluator\u001b[38;5;241m=\u001b[39mevaluator_auc, numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Step 6: Train the models with Cross-Validation\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mcv_lr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m dt_model \u001b[38;5;241m=\u001b[39m cv_dt\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m     59\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m cv_rf\u001b[38;5;241m.\u001b[39mfit(train_data)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3286.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 74.0 failed 1 times, most recent failure: Lost task 8.0 in stage 74.0 (TID 546) (23b0cefcebcd executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$4992/0x00007f46fd2e23e0`: (struct<UniqueCarrier_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,ORIGIN_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEST_Vec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:double,ARR_DELAY:double,CRS_ELAPSED_TIME:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Step 1: Split data into training and test sets (70% train, 30% test)\n",
        "train_data, test_data = df_prepared.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Remove rows with null values in the selected columns\n",
        "train_data = train_data.dropna(subset=['UniqueCarrier_Vec', 'ORIGIN_Vec', 'DEST_Vec', 'DEP_DELAY', 'ARR_DELAY', 'CRS_ELAPSED_TIME'])\n",
        "\n",
        "# Step 2: Define models\n",
        "lr = LogisticRegression(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "dt = DecisionTreeClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "rf = RandomForestClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "gbt = GBTClassifier(labelCol=\"CANCELLED\", featuresCol=\"features\")\n",
        "\n",
        "# Step 3: Define the evaluation metrics\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"CANCELLED\", metricName=\"areaUnderROC\")\n",
        "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"CANCELLED\", metricName=\"accuracy\")\n",
        "\n",
        "# Step 4: Define Hyperparameter Grid for Cross-Validation\n",
        "# Logistic Regression Hyperparameter Grid\n",
        "lr_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Decision Tree Hyperparameter Grid\n",
        "dt_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n",
        "    .addGrid(dt.maxBins, [32, 64]) \\\n",
        "    .build()\n",
        "\n",
        "# Random Forest Hyperparameter Grid\n",
        "rf_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
        "    .build()\n",
        "\n",
        "# Gradient Boosted Tree Hyperparameter Grid\n",
        "gbt_param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxIter, [10, 50, 100]) \\\n",
        "    .addGrid(gbt.maxDepth, [3, 5, 7]) \\\n",
        "    .build()\n",
        "\n",
        "# Step 5: Setup Cross-Validator for Hyperparameter Tuning\n",
        "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=lr_param_grid, evaluator=evaluator_auc, numFolds=3)\n",
        "cv_dt = CrossValidator(estimator=dt, estimatorParamMaps=dt_param_grid, evaluator=evaluator_auc, numFolds=3)\n",
        "cv_rf = CrossValidator(estimator=rf, estimatorParamMaps=rf_param_grid, evaluator=evaluator_auc, numFolds=3)\n",
        "cv_gbt = CrossValidator(estimator=gbt, estimatorParamMaps=gbt_param_grid, evaluator=evaluator_auc, numFolds=3)\n",
        "\n",
        "# Step 6: Train the models with Cross-Validation\n",
        "lr_model = cv_lr.fit(train_data)\n",
        "dt_model = cv_dt.fit(train_data)\n",
        "rf_model = cv_rf.fit(train_data)\n",
        "gbt_model = cv_gbt.fit(train_data)\n",
        "\n",
        "# Step 7: Make predictions on test data\n",
        "lr_predictions = lr_model.transform(test_data)\n",
        "dt_predictions = dt_model.transform(test_data)\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "gbt_predictions = gbt_model.transform(test_data)\n",
        "\n",
        "# Step 8: Evaluate models using Accuracy and AUC\n",
        "# Accuracy\n",
        "lr_accuracy = evaluator_accuracy.evaluate(lr_predictions)\n",
        "dt_accuracy = evaluator_accuracy.evaluate(dt_predictions)\n",
        "rf_accuracy = evaluator_accuracy.evaluate(rf_predictions)\n",
        "gbt_accuracy = evaluator_accuracy.evaluate(gbt_predictions)\n",
        "\n",
        "# AUC\n",
        "lr_auc = evaluator_auc.evaluate(lr_predictions)\n",
        "dt_auc = evaluator_auc.evaluate(dt_predictions)\n",
        "rf_auc = evaluator_auc.evaluate(rf_predictions)\n",
        "gbt_auc = evaluator_auc.evaluate(gbt_predictions)\n",
        "\n",
        "# Step 9: Print the results\n",
        "print(\"Logistic Regression Accuracy: \", lr_accuracy)\n",
        "print(\"Decision Tree Accuracy: \", dt_accuracy)\n",
        "print(\"Random Forest Accuracy: \", rf_accuracy)\n",
        "print(\"GBT Accuracy: \", gbt_accuracy)\n",
        "\n",
        "print(\"Logistic Regression AUC: \", lr_auc)\n",
        "print(\"Decision Tree AUC: \", dt_auc)\n",
        "print(\"Random Forest AUC: \", rf_auc)\n",
        "print(\"GBT AUC: \", gbt_auc)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "Practice session - Dataframe",
      "notebookOrigID": 1061204080530756,
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}